{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SE3M_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ0v0E_ANbSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep learning arquitecture - SE3M\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import KFold \n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "df = pd.read_csv('/content/gdrive/My Drive/data/reqTxt.csv', header=None) #Report or file containing the set of training and test texts.\n",
        "dfRequire = df.iloc[:,:] \n",
        "print(dfRequire.shape)\n",
        "print(dfRequire.columns)\n",
        "X = dfRequire[0]\n",
        "print(X[0])\n",
        "X = np.array(X)\n",
        "\n",
        "print(len(X))\n",
        "\n",
        "print('Train and test dataset loaded...')\n",
        "\n",
        "y = pd.read_csv('/content/gdrive/My Drive/data/estiDeep.data', header=None) #File containing the set of training and test labels.\n",
        "y = np.array(y)\n",
        "print ('Shape of label tensor:', y.shape)\n",
        "print(y.dtype)\n",
        "\n",
        "#Number of texts in train and test dataset \n",
        "MAX_LEN = 23313\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1000) \n",
        "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
        "print(kf) \n",
        "\n",
        "# Load Word2Vec sentende embeddings - generic\n",
        "#pret_model = pd.read_csv('/content/gdrive/My Drive/pretrain_model/word2vec_base.csv', delimiter= ',', sep= ',', header = None)  #Usado esse como Wiki medio genérico\n",
        "\n",
        "# Load Word2Vec sentende embeddings - fine-tuning\n",
        "#pret_model = pd.read_csv('/content/gdrive/My Drive/pretrain_model/word2vec_SE.csv', delimiter= ',', sep= ',', header=None) #embedding word2vec Wiki fine-tuning com o dataset de pré-treino\n",
        "\n",
        "# Load BERT sentence embeddings - generic\n",
        "#pret_model = pd.read_csv('/content/gdrive/My Drive/pretrain_model/BERT_base.csv', delimiter= ',', sep= ',', header=None) #novo bert as service\n",
        "\n",
        "# Load BERT sentence embeddings - fine-tuning\n",
        "pret_model = pd.read_csv('/content/gdrive/My Drive/pretrain_model/BERT_SE.csv', delimiter= ',', sep= ',', header=None) #novo bert as service\n",
        "\n",
        "embedding_matrix = pret_model.iloc[0:23313,:] # for BERT models\n",
        "###embedding_matrix = pret_model.iloc[:,1:101] # for word2vec_base\n",
        "###embedding_matrix = pret_model.iloc[1:,:] # for word2vec_SE\n",
        "\n",
        "dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
        "embedding_mat = dfEmbedding_mat.fillna('0') \n",
        "\n",
        "print('Embedding mat: ' + str(embedding_mat.shape))\n",
        "\n",
        "vetMAE = []\n",
        "vetR2 = []\n",
        "vetMSE = []\n",
        "vetMdae = []\n",
        "\n",
        "i = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "\n",
        "    x_train, test_x = X[train_index], X[test_index]\n",
        "    train_y, test_y = y[train_index], y[test_index]\n",
        "    \n",
        "    # get the raw text data\n",
        "    texts_train = x_train.astype(str)\n",
        "    texts_test = test_x.astype(str)\n",
        "\n",
        "    # vectorize the text samples                                   \n",
        "    tokenizer = Tokenizer(num_words = MAX_LEN, char_level=False, lower=False) \n",
        "    tokenizer.fit_on_texts(texts_train)                            \n",
        "    encSequences = tokenizer.texts_to_sequences(texts_train)          \n",
        "    encSequences_test = tokenizer.texts_to_sequences(texts_test)      \n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1 \n",
        "    print('Vocab_size: '+ str(vocab_size))\n",
        "\n",
        "    MAX_SEQUENCE_LENGTH = 100 #number of words in each text\n",
        "\n",
        "    x_train = pad_sequences(encSequences, maxlen= MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    x_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    print('Shape of data tensor:', x_train.shape)\n",
        "    print('Shape of data test tensor:', x_test.shape)\n",
        "\n",
        "    print('train_y: ' + str(train_y.shape))\n",
        "    print('test_y: ' + str(test_y.shape))\n",
        "\n",
        "\n",
        "    #Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding = Embedding(MAX_LEN, 768, input_length = 100, name='embedding', trainable=True) #for BERT pre-trained model\n",
        "    ###embedding = Embedding(MAX_LEN, 100, input_length = 100, name='embedding') #for word2vec pre-trained model\n",
        "    embedding.build(input_shape=(1,)) # the input_shape here has no effect in the build function\n",
        "    embedding.set_weights([embedding_mat])\n",
        "    model.add(embedding)\n",
        "    \n",
        "    model.add(AveragePooling1D(pool_size=100))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    #Enable this line for the model with LSTM\n",
        "    ##model.add(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=False)) \n",
        "    \n",
        "    #Disable the line below for the model with the LSTM layer\n",
        "    model.add(Flatten()) \n",
        "    \n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu')) \n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    adam = Adam(lr = 0.001, beta_1 = 0.99, beta_2 = 0.999, epsilon = None, decay = 0.01, amsgrad = False)\n",
        "\n",
        "    model.compile(loss = 'mse', optimizer= 'adam', metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    print('Modelo compilado...')\n",
        "\n",
        "    es = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "    model_history = model.fit(x_train, train_y,\n",
        "              batch_size= 128,\n",
        "              epochs=30, callbacks=[es],\n",
        "              validation_data=(x_test, test_y))                \n",
        "\n",
        "    y_pred = model.predict(x_test, batch_size=None, verbose=0, steps=None)\n",
        "    x_pred = model.predict(x_train, batch_size=None, verbose=0, steps=None)\n",
        "\n",
        "\n",
        "    #Metrics\n",
        "    print(\"\\n\")\n",
        "    mae = mean_absolute_error(test_y, y_pred)\n",
        "    vetMAE.append(mae)\n",
        "    print(\"MAE: %f\" % (mae))\n",
        "    medAE = median_absolute_error(test_y, y_pred)\n",
        "    vetMdae.append(medAE)\n",
        "    print(\"MedAE: %f\" % (medAE))\n",
        "    r2 = r2_score(test_y, y_pred, multioutput='raw_values')\n",
        "    vetR2.append(r2)\n",
        "    print(\"r2: %f\" % (r2))\n",
        "    mse = mean_squared_error(test_y, y_pred)\n",
        "    vetMSE.append(mse)\n",
        "    print(\"MSE: %f\" % (mse))\n",
        "    mErr = max_error(test_y, y_pred)\n",
        "    print(\"maxrror: %f\" % (mErr))\n",
        "\n",
        "    i = i + 1\n",
        "    print(\"Concluido \" + str(i))\n",
        "\n",
        "maeMedio = np.mean(vetMAE)  \n",
        "madAEMedio = np.mean(vetMdae)  \n",
        "r2Medio = np.mean(vetR2)  \n",
        "mseMedio = np.mean(vetMSE) \n",
        "stdMae = np.std(vetMAE)\n",
        "stdr2 = np.std(vetR2)\n",
        "stdMse = np.std(vetMSE)\n",
        "\n",
        "print('maeMedio: ' + str(maeMedio))\n",
        "print('madAEMedio: ' + str(madAEMedio))\n",
        "print('r2Medio: ' + str(r2Medio))\n",
        "print('mseMedio: ' + str(mseMedio))\n",
        "print('stdMae: ' + str(stdMae))\n",
        "print('stdr2: ' + str(stdr2))\n",
        "print('stdMse: ' + str(stdMse))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}